<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>skspec.correlation.pcakernel &mdash; scikit-spectra 0.3.1-2 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '0.3.1-2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="author" title="About these documents" href="../../../about.html" />
    <link rel="top" title="scikit-spectra 0.3.1-2 documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" /> 
  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
        <div class="headertitle"><a
          href="../../../index.html">scikit-spectra 0.3.1-2 documentation</a></div>
        <div class="rel">
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>


<a href="https://github.com/hugadams/scikit-spectra"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/365986a132ccd6a44c23a9169022c0b5c890c387/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f7265645f6161303030302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"></a>

<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="../../../index.html"><img src="../../../_static/new_logo.png" border="100" alt="sampledoc"/></a>
</div>


    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <h1>Source code for skspec.correlation.pcakernel</h1><div class="highlight"><pre>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Adapted to fit skspec 5/6/2013.  Original credit to Alexis Mignon:</span>

<span class="sd">Module for Principal Component Analysis.</span>

<span class="sd">Features:</span>
<span class="sd">* pca and kernel pca</span>
<span class="sd">* pca through singular value decomposition (SVD)</span>

<span class="sd">Author: Alexis Mignon (c)</span>
<span class="sd">Date: 10/01/2012</span>
<span class="sd">e-mail: alexis.mignon@gmail.com</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.sparse.linalg.eigen.arpack</span> <span class="kn">import</span> <span class="n">eigs</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">eigh</span>

<div class="viewcode-block" id="full_pca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.full_pca">[docs]</a><span class="k">def</span> <span class="nf">full_pca</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the complete eigen decomposition of</span>
<span class="sd">    the covariance matrix.</span>
<span class="sd">    </span>
<span class="sd">    arguments:</span>
<span class="sd">    * data: 2D numpy array where each row is a sample and</span>
<span class="sd">        each column a feature.</span>
<span class="sd">    </span>
<span class="sd">    return:</span>
<span class="sd">    * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">          highest to lowest.</span>
<span class="sd">    * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">         corresponding to w[i]</span>
<span class="sd">         </span>
<span class="sd">    Notes: If you want to compute only a few number of principal</span>
<span class="sd">           components, you should consider using &#39;pca&#39; or &#39;svd_pca&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span><span class="n">overwrite_a</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">u</span><span class="p">[:,::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</div>
<div class="viewcode-block" id="pca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.pca">[docs]</a><span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the eigen decomposition of the covariance matrix.</span>
<span class="sd">    </span>
<span class="sd">    arguments:</span>
<span class="sd">    * data: 2D numpy array where each row is a sample and</span>
<span class="sd">            each column a feature.</span>
<span class="sd">    * k: number of principal components to keep.</span>
<span class="sd">    </span>
<span class="sd">    return:</span>
<span class="sd">    * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">          highest to lowest.</span>
<span class="sd">    * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">         corresponding to w[i]</span>
<span class="sd">         </span>
<span class="sd">    Notes: If the number of samples is much smaller than the number</span>
<span class="sd">           of features, you should consider the use of &#39;svd_pca&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c"># kw &quot;which&quot; means return largest magnitude k eigenvalues</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigs</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span><span class="n">which</span> <span class="o">=</span> <span class="s">&#39;LM&#39;</span><span class="p">)</span>
<span class="c">#    return w[::-1],u[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="c">#(No need to reverse w,u because eigs does it using &#39;LM&#39;)</span>
</div>
<div class="viewcode-block" id="extern_pca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.extern_pca">[docs]</a><span class="k">def</span> <span class="nf">extern_pca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the eigen decomposition of the covariance matrix based</span>
<span class="sd">    on the eigen decomposition of the exterior product matrix.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    arguments:</span>
<span class="sd">    * data: 2D numpy array where each row is a sample and</span>
<span class="sd">            each column a feature.</span>
<span class="sd">    * k: number of principal components to keep.</span>
<span class="sd">    </span>
<span class="sd">    return:</span>
<span class="sd">    * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">          highest to lowest.</span>
<span class="sd">    * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">         corresponding to w[i]</span>
<span class="sd">         </span>
<span class="sd">    Notes: This function computes PCA, based on the exterior product</span>
<span class="sd">           matrix (C = X*X.T/(n-1)) instead of the covariance matrix</span>
<span class="sd">           (C = X.T*X) and uses relations based of the singular</span>
<span class="sd">           value decomposition to compute the corresponding the</span>
<span class="sd">           final eigen vectors. While this can be much faster when </span>
<span class="sd">           the number of samples is much smaller than the number</span>
<span class="sd">           of features, it can lead to loss of precisions.</span>
<span class="sd">           </span>
<span class="sd">           The (centered) data matrix X can be decomposed as:</span>
<span class="sd">                X.T = U * S * v.T</span>
<span class="sd">           On computes the eigen decomposition of :</span>
<span class="sd">                X * X.T = v*S^2*v.T</span>
<span class="sd">           and the eigen vectors of the covariance matrix are</span>
<span class="sd">           computed as :</span>
<span class="sd">                U = X.T * v * S^(-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
   
<span class="c">#    raise NotImplementedError(&#39;Need to curate this method&#39;)</span>
    <span class="c"># Should I take last line and remove the [::-1] stuff?</span>
    <span class="n">data_m</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data_m</span><span class="p">,</span><span class="n">data_m</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">eigs</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span><span class="n">which</span> <span class="o">=</span> <span class="s">&#39;LM&#39;</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">v</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="c"># Normalizes eigenvalues by length of data (?)</span>
<span class="c">#    return w[::-1]/(len(data)-1),U[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">U</span>
</div>
<div class="viewcode-block" id="full_kpca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.full_kpca">[docs]</a><span class="k">def</span> <span class="nf">full_kpca</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the complete eigen decomposition of a kernel matrix.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * data: 2D numpy array representing the symmetric kernel matrix.</span>
<span class="sd">        </span>
<span class="sd">        return:</span>
<span class="sd">        * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">              highest to lowest.</span>
<span class="sd">        * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">             corresponding to w[i]</span>
<span class="sd">             </span>
<span class="sd">        Notes: If you want to compute only a few number of principal</span>
<span class="sd">               components, you should consider using &#39;kpca&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigh</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">overwrite_a</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c">#    return w[::-1],u[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span><span class="n">u</span>

</div>
<div class="viewcode-block" id="kpca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.kpca">[docs]</a><span class="k">def</span> <span class="nf">kpca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the eigen decomposition of the kernel matrix.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * data: 2D numpy array representing the symmetric kernel matrix.</span>
<span class="sd">        * k: number of principal components to keep.</span>
<span class="sd">        </span>
<span class="sd">        return:</span>
<span class="sd">        * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">              highest to lowest.</span>
<span class="sd">        * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">             corresponding to w[i]</span>
<span class="sd">             </span>
<span class="sd">        Notes: If you want to perform the full decomposition, consider </span>
<span class="sd">               using &#39;full_kpca&#39; instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigs</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span><span class="n">which</span> <span class="o">=</span> <span class="s">&#39;LM&#39;</span><span class="p">)</span>
<span class="c">#    return w[::-1],u[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span><span class="n">u</span>
</div>
<div class="viewcode-block" id="PCA"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.PCA">[docs]</a><span class="k">class</span> <span class="nc">PCA</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        PCA object to perform Principal Component Analysis.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">extern</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * k: number of principal components to compute. &#39;None&#39;</span>
<span class="sd">             (default) means that all components are computed.</span>
<span class="sd">        * kernel: perform PCA on kernel matrices (default is False)</span>
<span class="sd">        * extern: use extern product to perform PCA (default is </span>
<span class="sd">               False). Use this option when the number of samples</span>
<span class="sd">               is much smaller than the number of features.</span>

<span class="sd">        Notes:</span>
<span class="sd">        * All data will be mean-cenetered.  Np subroutines (eg np.cov())</span>
<span class="sd">          do this in all cases except for the extern_pca() method, which</span>
<span class="sd">          does this automatically.</span>
<span class="sd">        &quot;&quot;&quot;</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">_k</span> <span class="o">=</span> <span class="n">k</span> <span class="c">#THESE SHOULD BE PROPERTIES</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_extern</span> <span class="o">=</span> <span class="n">extern</span>
        
    
<div class="viewcode-block" id="PCA.fit"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.PCA.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs PCA on the data array X.</span>
<span class="sd">        arguments:</span>
<span class="sd">        * X: 2D numpy array. In case the array represents a kernel</span>
<span class="sd">             matrix, X should be symmetric. Otherwise each row</span>
<span class="sd">             represents a sample and each column represents a</span>
<span class="sd">             feature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c"># Mean centering is inherently performed in numpy methods that compute</span>
        <span class="c"># the covariance.  This does it additionally because X may not be</span>
        <span class="c"># the same matrix used to fit the original data</span>
        
        <span class="c"># If number principle componenets is not specified, full pca or kpca</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k</span> <span class="ow">is</span> <span class="bp">None</span> <span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">full_kpca</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">full_pca</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span> <span class="o">=</span> <span class="n">pca_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c"># If specifying number of principle components</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">kpca</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extern</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">extern_pca</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">pca</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span> <span class="o">=</span> <span class="n">pca_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">_k</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
            
            <span class="n">total_variance</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            
        <span class="c"># If not kernal pca, subtract mean.</span>
        <span class="c"># Total variance is squared difference from mean, since mean centered.</span>
        <span class="c"># Simply computes variance of each element</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
            <span class="n">total_variance</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff</span><span class="o">*</span><span class="n">diff</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c"># Scikit image also has this; how much variance each component</span>
        <span class="c"># can account for.  Should sum to 1 if all components used.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explained_variance_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">total_variance</span>
        <span class="k">return</span> <span class="bp">self</span>
    
        </div>
<div class="viewcode-block" id="PCA.transform"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.PCA.transform">[docs]</a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">whiten</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Project data on the principal components. If the whitening</span>
<span class="sd">        option is used, components will be normalized to that they</span>
<span class="sd">        have the same contribution.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * X: 2D numpy array of data to project.</span>
<span class="sd">        * whiten: (default is False) all components are normalized</span>
<span class="sd">            so that they have the same contribution.</span>
<span class="sd">            </span>
<span class="sd">        returns:</span>
<span class="sd">        * prX : projection of X on the principal components.</span>
<span class="sd">        </span>
<span class="sd">        Notes: In the case of Kernel PCA, X[i] represents the value</span>
<span class="sd">           of the kernel between sample i and the j-th sample used</span>
<span class="sd">           at train time. Thus, if fit was called with a NxN kernel</span>
<span class="sd">           matrix, X should be a MxN matrix.</span>
<span class="sd">           </span>
<span class="sd">           The projection in the kernel case is made to be equivalent</span>
<span class="sd">           to the projection in the linear case.</span>
<span class="sd">           </span>
<span class="sd">               X.T = U * S * v.T</span>
<span class="sd">               C = 1/(N-1) * X.T * X</span>
<span class="sd">               X.T * X = U*S^2*U.T</span>
<span class="sd">               K = X * X.T = v*S^2*v.T</span>
<span class="sd">               </span>
<span class="sd">               U = X.T * v * S^(-1)</span>
<span class="sd">           </span>
<span class="sd">           The projection with PCA is :</span>
<span class="sd">               X&#39; = X * U</span>
<span class="sd">               X&#39; = X * X.T * v * S^(-1)</span>
<span class="sd">               X&#39; = K * v * S^(-1)</span>
<span class="sd">               </span>
<span class="sd">           For whiten PCA :</span>
<span class="sd">               X&#39; = X * U * S^(-1) * sqrt(N-1)</span>
<span class="sd">               X&#39; = X * X.T * v * S^(-1) * S^(-1) * sqrt(N-1)</span>
<span class="sd">               X&#39; = K * S^(-2) * sqrt(N-1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
    
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
            <span class="n">pr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">whiten</span> <span class="p">:</span>
                <span class="n">pr</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">pr</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">)</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="n">pr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">whiten</span><span class="p">:</span>
                <span class="n">pr</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pr</span>
    
        </div></div>
</pre></div>

          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          <h3>Table Of Contents</h3>
          <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../API/APIMAIN.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about.html">History and Background</a></li>
</ul>

          <h3 style="margin-top: 1.5em;">Search</h3>
          <form class="search" action="../../../search.html" method="get">
            <input type="text" name="q" />
            <input type="submit" value="Go" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
          </form>
          <p class="searchtip" style="font-size: 90%">
            Enter search terms or a module, class or function name.
          </p>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../../../genindex.html" title="General Index"
             >index</a>
        </div>

        <div class="right">
          
    <div class="footer">
        &copy; Copyright 2014, Adam Hughes, ReevesLab Dept. Physics, George Washington University.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>