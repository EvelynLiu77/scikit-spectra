<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>skspec.correlation.pcakernel &mdash; scikit-spectra 0.3.1-2 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootswatch-3.2.0/united/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '0.3.1-2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-3.2.0/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../../_static/bootstrap-sphinx.js"></script>
    <link rel="author" title="About these documents" href="../../../about.html" />
    <link rel="top" title="scikit-spectra 0.3.1-2 documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>

<a href="https://github.com/hugadams/scikit-spectra"
class="visible-desktop hidden-xs"><img
id="gh-banner"
style="position: absolute; top: 50px; right: 0; border: 0;"
src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"
alt="Fork me on GitHub"></a>
<script>
// Adjust banner height.
$(function () {
var navHeight = $(".navbar .container").css("height");
$("#gh-banner").css("top", navHeight);
});
</script>



<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="../../../index.html"><img src="../../../_static/logo.png" border="100" alt="sampledoc"/></a>
</div>




<a href="https://github.com/hugadams/scikit-spectra"
class="visible-desktop hidden-xs"><img
id="gh-banner"
style="position: absolute; top: 50px; right: 0; border: 0;"
src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"
alt="Fork me on GitHub"></a>
<script>
// Adjust banner height.
$(function () {
var navHeight = $(".navbar .container").css("height");
$("#gh-banner").css("top", navHeight);
});
</script>





  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html">
          skspec</a>
        <span class="navbar-text navbar-version pull-left"><b>0.3</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../examples.html">Examples</a></li>
                <li><a href="http://example.com">Link</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../index.html">Jump to <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../API/APIMAIN.html">API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#primary-structures-spectrum-spectra-specstack">Primary Structures: Spectrum, Spectra, SpecStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#index-objects">Index Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#builtin-datasets">Builtin Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#io">IO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#plotting">Plotting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#graphical-user-interface-gui">Graphical User Interface (GUI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../API/APIMAIN.html#two-dimensional-correlation-spectroscopy">Two-Dimensional Correlation Spectroscopy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../support.html">Support</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../support.html#mailing-list">Mailing List</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../support.html#contribute">Contribute</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials.html#videos">Videos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials.html#minutes-to-scikit-spectra">10 Minutes to scikit-spectra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials.html#ipython-notebooks">IPython Notebooks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#dependencies">Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#pip-install">Pip Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#installation-from-source">Installation from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#testing-installation">Testing Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../about.html">About</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../about.html#history-and-background">History and Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../about.html#related-scipy-libraries">Related Scipy Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../about.html#other-spectroscopy-libraries-tools-in-python">Other Spectroscopy Libraries/Tools in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../about.html#about-the-author">About the Author</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../about.html#acknowledgements">Acknowledgements</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">This Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
<form action="../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-12">
      
  <h1>Source code for skspec.correlation.pcakernel</h1><div class="highlight"><pre>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Adapted to fit skspec 5/6/2013.  Original credit to Alexis Mignon:</span>

<span class="sd">Module for Principal Component Analysis.</span>

<span class="sd">Features:</span>
<span class="sd">* pca and kernel pca</span>
<span class="sd">* pca through singular value decomposition (SVD)</span>

<span class="sd">Author: Alexis Mignon (c)</span>
<span class="sd">Date: 10/01/2012</span>
<span class="sd">e-mail: alexis.mignon@gmail.com</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.sparse.linalg.eigen.arpack</span> <span class="kn">import</span> <span class="n">eigs</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">eigh</span>

<div class="viewcode-block" id="full_pca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.full_pca">[docs]</a><span class="k">def</span> <span class="nf">full_pca</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the complete eigen decomposition of</span>
<span class="sd">    the covariance matrix.</span>
<span class="sd">    </span>
<span class="sd">    arguments:</span>
<span class="sd">    * data: 2D numpy array where each row is a sample and</span>
<span class="sd">        each column a feature.</span>
<span class="sd">    </span>
<span class="sd">    return:</span>
<span class="sd">    * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">          highest to lowest.</span>
<span class="sd">    * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">         corresponding to w[i]</span>
<span class="sd">         </span>
<span class="sd">    Notes: If you want to compute only a few number of principal</span>
<span class="sd">           components, you should consider using &#39;pca&#39; or &#39;svd_pca&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span><span class="n">overwrite_a</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">u</span><span class="p">[:,::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</div>
<div class="viewcode-block" id="pca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.pca">[docs]</a><span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the eigen decomposition of the covariance matrix.</span>
<span class="sd">    </span>
<span class="sd">    arguments:</span>
<span class="sd">    * data: 2D numpy array where each row is a sample and</span>
<span class="sd">            each column a feature.</span>
<span class="sd">    * k: number of principal components to keep.</span>
<span class="sd">    </span>
<span class="sd">    return:</span>
<span class="sd">    * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">          highest to lowest.</span>
<span class="sd">    * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">         corresponding to w[i]</span>
<span class="sd">         </span>
<span class="sd">    Notes: If the number of samples is much smaller than the number</span>
<span class="sd">           of features, you should consider the use of &#39;svd_pca&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c"># kw &quot;which&quot; means return largest magnitude k eigenvalues</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigs</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span><span class="n">which</span> <span class="o">=</span> <span class="s">&#39;LM&#39;</span><span class="p">)</span>
<span class="c">#    return w[::-1],u[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="c">#(No need to reverse w,u because eigs does it using &#39;LM&#39;)</span>
</div>
<div class="viewcode-block" id="extern_pca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.extern_pca">[docs]</a><span class="k">def</span> <span class="nf">extern_pca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the eigen decomposition of the covariance matrix based</span>
<span class="sd">    on the eigen decomposition of the exterior product matrix.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    arguments:</span>
<span class="sd">    * data: 2D numpy array where each row is a sample and</span>
<span class="sd">            each column a feature.</span>
<span class="sd">    * k: number of principal components to keep.</span>
<span class="sd">    </span>
<span class="sd">    return:</span>
<span class="sd">    * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">          highest to lowest.</span>
<span class="sd">    * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">         corresponding to w[i]</span>
<span class="sd">         </span>
<span class="sd">    Notes: This function computes PCA, based on the exterior product</span>
<span class="sd">           matrix (C = X*X.T/(n-1)) instead of the covariance matrix</span>
<span class="sd">           (C = X.T*X) and uses relations based of the singular</span>
<span class="sd">           value decomposition to compute the corresponding the</span>
<span class="sd">           final eigen vectors. While this can be much faster when </span>
<span class="sd">           the number of samples is much smaller than the number</span>
<span class="sd">           of features, it can lead to loss of precisions.</span>
<span class="sd">           </span>
<span class="sd">           The (centered) data matrix X can be decomposed as:</span>
<span class="sd">                X.T = U * S * v.T</span>
<span class="sd">           On computes the eigen decomposition of :</span>
<span class="sd">                X * X.T = v*S^2*v.T</span>
<span class="sd">           and the eigen vectors of the covariance matrix are</span>
<span class="sd">           computed as :</span>
<span class="sd">                U = X.T * v * S^(-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
   
<span class="c">#    raise NotImplementedError(&#39;Need to curate this method&#39;)</span>
    <span class="c"># Should I take last line and remove the [::-1] stuff?</span>
    <span class="n">data_m</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data_m</span><span class="p">,</span><span class="n">data_m</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">eigs</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span><span class="n">which</span> <span class="o">=</span> <span class="s">&#39;LM&#39;</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">v</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="c"># Normalizes eigenvalues by length of data (?)</span>
<span class="c">#    return w[::-1]/(len(data)-1),U[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">U</span>
</div>
<div class="viewcode-block" id="full_kpca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.full_kpca">[docs]</a><span class="k">def</span> <span class="nf">full_kpca</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the complete eigen decomposition of a kernel matrix.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * data: 2D numpy array representing the symmetric kernel matrix.</span>
<span class="sd">        </span>
<span class="sd">        return:</span>
<span class="sd">        * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">              highest to lowest.</span>
<span class="sd">        * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">             corresponding to w[i]</span>
<span class="sd">             </span>
<span class="sd">        Notes: If you want to compute only a few number of principal</span>
<span class="sd">               components, you should consider using &#39;kpca&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigh</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">overwrite_a</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c">#    return w[::-1],u[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span><span class="n">u</span>

</div>
<div class="viewcode-block" id="kpca"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.kpca">[docs]</a><span class="k">def</span> <span class="nf">kpca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs the eigen decomposition of the kernel matrix.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * data: 2D numpy array representing the symmetric kernel matrix.</span>
<span class="sd">        * k: number of principal components to keep.</span>
<span class="sd">        </span>
<span class="sd">        return:</span>
<span class="sd">        * w: the eigen values of the covariance matrix sorted in from </span>
<span class="sd">              highest to lowest.</span>
<span class="sd">        * u: the corresponding eigen vectors. u[:,i] is the vector</span>
<span class="sd">             corresponding to w[i]</span>
<span class="sd">             </span>
<span class="sd">        Notes: If you want to perform the full decomposition, consider </span>
<span class="sd">               using &#39;full_kpca&#39; instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span><span class="p">,</span><span class="n">u</span> <span class="o">=</span> <span class="n">eigs</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span><span class="n">which</span> <span class="o">=</span> <span class="s">&#39;LM&#39;</span><span class="p">)</span>
<span class="c">#    return w[::-1],u[:,::-1]</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span><span class="n">u</span>
</div>
<div class="viewcode-block" id="PCA"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.PCA">[docs]</a><span class="k">class</span> <span class="nc">PCA</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        PCA object to perform Principal Component Analysis.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">extern</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * k: number of principal components to compute. &#39;None&#39;</span>
<span class="sd">             (default) means that all components are computed.</span>
<span class="sd">        * kernel: perform PCA on kernel matrices (default is False)</span>
<span class="sd">        * extern: use extern product to perform PCA (default is </span>
<span class="sd">               False). Use this option when the number of samples</span>
<span class="sd">               is much smaller than the number of features.</span>

<span class="sd">        Notes:</span>
<span class="sd">        * All data will be mean-cenetered.  Np subroutines (eg np.cov())</span>
<span class="sd">          do this in all cases except for the extern_pca() method, which</span>
<span class="sd">          does this automatically.</span>
<span class="sd">        &quot;&quot;&quot;</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">_k</span> <span class="o">=</span> <span class="n">k</span> <span class="c">#THESE SHOULD BE PROPERTIES</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_extern</span> <span class="o">=</span> <span class="n">extern</span>
        
    
<div class="viewcode-block" id="PCA.fit"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.PCA.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs PCA on the data array X.</span>
<span class="sd">        arguments:</span>
<span class="sd">        * X: 2D numpy array. In case the array represents a kernel</span>
<span class="sd">             matrix, X should be symmetric. Otherwise each row</span>
<span class="sd">             represents a sample and each column represents a</span>
<span class="sd">             feature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c"># Mean centering is inherently performed in numpy methods that compute</span>
        <span class="c"># the covariance.  This does it additionally because X may not be</span>
        <span class="c"># the same matrix used to fit the original data</span>
        
        <span class="c"># If number principle componenets is not specified, full pca or kpca</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_k</span> <span class="ow">is</span> <span class="bp">None</span> <span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">full_kpca</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">full_pca</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span> <span class="o">=</span> <span class="n">pca_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c"># If specifying number of principle components</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">kpca</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extern</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">extern_pca</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">pca_func</span> <span class="o">=</span> <span class="n">pca</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span> <span class="o">=</span> <span class="n">pca_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">_k</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
            
            <span class="n">total_variance</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">diagonal</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            
        <span class="c"># If not kernal pca, subtract mean.</span>
        <span class="c"># Total variance is squared difference from mean, since mean centered.</span>
        <span class="c"># Simply computes variance of each element</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
            <span class="n">total_variance</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff</span><span class="o">*</span><span class="n">diff</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c"># Scikit image also has this; how much variance each component</span>
        <span class="c"># can account for.  Should sum to 1 if all components used.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explained_variance_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">total_variance</span>
        <span class="k">return</span> <span class="bp">self</span>
    
        </div>
<div class="viewcode-block" id="PCA.transform"><a class="viewcode-back" href="../../../API/skspec.correlation.html#skspec.correlation.pcakernel.PCA.transform">[docs]</a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">whiten</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Project data on the principal components. If the whitening</span>
<span class="sd">        option is used, components will be normalized to that they</span>
<span class="sd">        have the same contribution.</span>
<span class="sd">        </span>
<span class="sd">        arguments:</span>
<span class="sd">        * X: 2D numpy array of data to project.</span>
<span class="sd">        * whiten: (default is False) all components are normalized</span>
<span class="sd">            so that they have the same contribution.</span>
<span class="sd">            </span>
<span class="sd">        returns:</span>
<span class="sd">        * prX : projection of X on the principal components.</span>
<span class="sd">        </span>
<span class="sd">        Notes: In the case of Kernel PCA, X[i] represents the value</span>
<span class="sd">           of the kernel between sample i and the j-th sample used</span>
<span class="sd">           at train time. Thus, if fit was called with a NxN kernel</span>
<span class="sd">           matrix, X should be a MxN matrix.</span>
<span class="sd">           </span>
<span class="sd">           The projection in the kernel case is made to be equivalent</span>
<span class="sd">           to the projection in the linear case.</span>
<span class="sd">           </span>
<span class="sd">               X.T = U * S * v.T</span>
<span class="sd">               C = 1/(N-1) * X.T * X</span>
<span class="sd">               X.T * X = U*S^2*U.T</span>
<span class="sd">               K = X * X.T = v*S^2*v.T</span>
<span class="sd">               </span>
<span class="sd">               U = X.T * v * S^(-1)</span>
<span class="sd">           </span>
<span class="sd">           The projection with PCA is :</span>
<span class="sd">               X&#39; = X * U</span>
<span class="sd">               X&#39; = X * X.T * v * S^(-1)</span>
<span class="sd">               X&#39; = K * v * S^(-1)</span>
<span class="sd">               </span>
<span class="sd">           For whiten PCA :</span>
<span class="sd">               X&#39; = X * U * S^(-1) * sqrt(N-1)</span>
<span class="sd">               X&#39; = X * X.T * v * S^(-1) * S^(-1) * sqrt(N-1)</span>
<span class="sd">               X&#39; = K * S^(-2) * sqrt(N-1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
    
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="p">:</span>
            <span class="n">pr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">whiten</span> <span class="p">:</span>
                <span class="n">pr</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="n">pr</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">)</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="n">pr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">whiten</span><span class="p">:</span>
                <span class="n">pr</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pr</span>
    
        </div></div>
</pre></div>

    </div>
    
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2014, Adam Hughes, ReevesLab, George Washington University.<br/>
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.2.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>